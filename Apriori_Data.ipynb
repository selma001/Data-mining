{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p15GuW0kp__T",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azi1uh5H6g2X"
      },
      "source": [
        "\n",
        "The *has_infrequent_subset* function plays a crucial role in optimizing the Apriori algorithm **by reducing the number of candidate itemsets that need to be considered**, therefore **lowering the number of database scan operations.**\n",
        "\n",
        "\n",
        "*   The function efficiently prunes candidate itemsets **by checking if any of their subsets are infrequent**. If any subset of a candidate itemset is infrequent, then the candidate itemset itself **cannot be frequent according to the Apriori property**.\n",
        "\n",
        "*   By avoiding the generation of candidate itemsets containing infrequent subsets, **the search space is significantly reduced**. This leads to fewer candidate itemsets to be considered for support counting, resulting in overall efficiency gains.\n",
        "\n",
        "\n",
        "\n",
        "*   Pruning candidate itemsets early in the process helps **conserve memory and processing resources**. It reduces the need to store and process large sets of candidate itemsets **that are unlikely to be frequent**.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrgOQD77t3rS",
        "outputId": "6d82470d-759b-4835-b46d-ad0232d3cc18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Bread']\n",
            " ['Scandinavian']\n",
            " ['Scandinavian']\n",
            " ...\n",
            " ['Coffee']\n",
            " ['Pastry']\n",
            " ['Smoothies']]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('bread-basket.csv')\n",
        "\n",
        "# Split the \"Item\" column by commas and convert it to a list of lists\n",
        "product_column = df[\"Item\"].str.split(',').tolist()\n",
        "\n",
        "# Find the maximum length of a transaction in the product_column\n",
        "max_length = max(len(transaction) for transaction in product_column)\n",
        "\n",
        "# Pad each transaction to have the same length using a placeholder value (e.g., 'None')\n",
        "padded_product_column = [transaction + [''] * (max_length - len(transaction)) for transaction in product_column]\n",
        "\n",
        "# Convert the padded_product_column to a NumPy array\n",
        "product_array = np.array(padded_product_column)\n",
        "\n",
        "# Example usage\n",
        "print(product_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WaCCV5gjqAAE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_candidate_itemsets(itemset, k):\n",
        "    \"\"\"\n",
        "    Generate candidate itemsets of size k from the given itemset.\n",
        "\n",
        "    1- Initialize an empty list candidate_itemsets to store the generated candidate itemsets.\n",
        "    2- Get the number of transactions (num_itemsets) in the itemset.\n",
        "    3- Iterate through each pair of transactions using two nested loops (i and j).\n",
        "    4- For each pair of transactions, check if their first k-1 items are the same. This is done using the condition itemset[i][:-1] == itemset[j][:-1].\n",
        "    5- If the first k-1 items are the same, join the sets of items from both transactions and sort them. This is done using the expression sorted(set(itemset[i]) | set(itemset[j])).\n",
        "    6- Check if the length of the resulting candidate itemset is equal to k.\n",
        "    7- If the length is equal to k, check if any subset of the candidate itemset is infrequent. This is done using the has_infrequent_subset function.\n",
        "    8- If none of the subsets are infrequent, append the candidate itemset to the candidate_itemsets list.\n",
        "    9- Repeat this process for all pairs of transactions.\n",
        "   10- Return the list of candidate itemsets.\n",
        "\n",
        "   yginiri les combinaisons, yvirifyi ldakhel ida kayen item machi frequent mayajoutihch to candidate itemset \n",
        "   \n",
        "    \"\"\"\n",
        "    candidate_itemsets = []\n",
        "    num_itemsets = len(itemset)\n",
        "\n",
        "    for i in range(num_itemsets):\n",
        "        for j in range(i + 1, num_itemsets):\n",
        "            if itemset[i][:-1] == itemset[j][:-1]:\n",
        "                candidate = sorted(set(itemset[i]) | set(itemset[j]))\n",
        "                if len(candidate) == k and '' not in candidate:\n",
        "                    if not has_infrequent_subset(candidate, itemset):\n",
        "                        candidate_itemsets.append(candidate)\n",
        "\n",
        "    return candidate_itemsets\n",
        "\n",
        "    \"\"\"\n",
        "    Checks whether all subsets of an itemset are frequent.\n",
        "\n",
        "    1- Use itertools.combinations to generate all possible subsets of the itemset with length len(itemset) - 1.\n",
        "    2- For each subset generated:\n",
        "      2.1- Convert it to a list and check if it exists in the list of prev_itemsets.\n",
        "      2.2- If any subset is not found in prev_itemsets, return True immediately, indicating that the itemset may contain infrequent subsets.\n",
        "    3- If all subsets are found in prev_itemsets, return False, indicating that the itemset contains only frequent subsets.\n",
        "\n",
        "    tvirfyi ida kayen item machi frequant\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "def has_infrequent_subset(itemset, prev_itemsets):\n",
        "    subsets = itertools.combinations(itemset, len(itemset) - 1)\n",
        "    for subset in subsets:\n",
        "        if list(subset) not in prev_itemsets:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JnNq1YYKqAAJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "    \"\"\"\n",
        "    Prune candidate itemsets that do not meet the minimum support threshold.\n",
        "\n",
        "    1- Initialize an empty list freq_itemsets to store frequent itemsets.\n",
        "    2- Initialize an empty dictionary item_counts to store the count of each candidate itemset.\n",
        "    3- Iterate through each transaction in the itemset.\n",
        "    4- For each transaction, iterate through each candidate itemset in candidate_itemsets.\n",
        "    5- If the candidate itemset is a subset of the transaction, update its count in the item_counts dictionary.\n",
        "    7- Iterate through each candidate itemset in candidate_itemsets.\n",
        "    8- Calculate the support for each candidate itemset by dividing its count by the total number of transactions.\n",
        "    9- If the support is greater than or equal to the min_support, add the candidate itemset to the freq_itemsets list.\n",
        "   10- Return the list of frequent itemsets.\n",
        "\n",
        "    \"\"\"\n",
        "    def prune_itemsets(itemset, candidate_itemsets, min_support, min_confidence):\n",
        "      freq_itemsets = []\n",
        "      item_counts = {}\n",
        "\n",
        "      # Count the occurrences of each candidate itemset\n",
        "      for transaction in itemset:\n",
        "          for candidate in candidate_itemsets:\n",
        "              if set(candidate).issubset(set(transaction)):\n",
        "                  item_counts[str(candidate)] = item_counts.get(str(candidate), 0) + 1\n",
        "\n",
        "      num_transactions = len(itemset)\n",
        "      for candidate in candidate_itemsets:\n",
        "          support = item_counts.get(str(candidate), 0) / num_transactions\n",
        "\n",
        "          # Check if the support meets the minimum support threshold\n",
        "          if support >= min_support:\n",
        "              freq_itemsets.append(candidate) \n",
        "\n",
        "              # Calculate confidence for association rules\n",
        "              for i in range(1, len(candidate)):\n",
        "                  antecedent = candidate[:i]\n",
        "                  consequent = candidate[i:]\n",
        "\n",
        "                  antecedent_support = item_counts.get(str(antecedent), 0) / num_transactions\n",
        "                  confidence = support / antecedent_support\n",
        "\n",
        "                  # Check if confidence meets the minimum confidence threshold\n",
        "                  if confidence >= min_confidence:\n",
        "                      freq_itemsets.append([antecedent, consequent, confidence])#change\n",
        "\n",
        "      return freq_itemsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OWPdLUVnqAAO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def apriori(itemset, min_support, min_conf):\n",
        "    \"\"\"\n",
        "    Implement Apriori algorithm to find frequent itemsets.\n",
        "\n",
        "    1- Initialize an empty list freq_itemsets to store frequent itemsets.\n",
        "    2- Initialize k to 1.\n",
        "    3- Extract unique items from itemset.\n",
        "    4- Convert each unique item into a singleton itemset.\n",
        "    5- Find frequent itemsets of size 1 by pruning using the prune_itemsets function.\n",
        "    6- Append the frequent itemsets of size 1 to freq_itemsets.\n",
        "    7- Iterate until no more frequent itemsets can be found:\n",
        "    8- Generate candidate itemsets of size k + 1 using the generate_candidate_itemsets function.\n",
        "    9- Find frequent itemsets of size k + 1 by pruning using the prune_itemsets function.\n",
        "   10- Increment k.\n",
        "   11- Append the frequent itemsets of size k + 1 to freq_itemsets.\n",
        "   12- Return the list of frequent itemsets, excluding the last empty list.\n",
        "\n",
        "    \"\"\"\n",
        "    freq_itemsets = []\n",
        "    k = 1\n",
        "\n",
        "    unique_items = set([item for sublist in itemset for item in sublist])\n",
        "    unique_itemsets = [[item] for item in unique_items]\n",
        "    freq_itemsets.append(prune_itemsets(itemset, unique_itemsets, min_support, min_conf))\n",
        "\n",
        "    while freq_itemsets[-1] != []:\n",
        "        candidate_itemsets = generate_candidate_itemsets(freq_itemsets[-1], k + 1)\n",
        "        freq_itemsets.append(prune_itemsets(itemset, candidate_itemsets, min_support, min_conf))\n",
        "        k += 1\n",
        "    #boucle w7doukhra tvirifyi lconfidence\n",
        "    return freq_itemsets[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q59LKDPJqAAX",
        "outputId": "efac3e96-f5d8-46f7-b696-50fcf1b255af",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequent Itemsets: [[['Brownie'], ['Bread'], ['Scone'], ['Cake'], ['Soup'], ['Juice'], ['Muffin'], ['Sandwich'], ['Farm House'], ['Pastry'], ['Toast'], ['Coffee'], ['Hot chocolate'], ['Scandinavian'], ['Cookies'], ['Tea'], ['Alfajores'], ['Medialuna']]]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example dataset\n",
        "\n",
        "    min_support = 0.01\n",
        "    min_confidence = 0.01\n",
        "    freq_itemsets = apriori(product_array, min_support, min_confidence)\n",
        "    print(\"Frequent Itemsets:\", freq_itemsets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "eOV4g4ovqAAa",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
